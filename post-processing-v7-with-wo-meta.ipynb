{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:25.840195Z",
     "start_time": "2019-10-28T16:33:25.833883Z"
    }
   },
   "outputs": [],
   "source": [
    "dp_model = \"resnet50-anjuu-512\"\n",
    "print(dp_model)\n",
    "patients_stacking_splits_filename = \"../csv/patients_stacking_splits.csv\"\n",
    "print(patients_stacking_splits_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:25.870648Z",
     "start_time": "2019-10-28T16:33:25.844005Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_filename = \"../submissions/Bac/oof_valid_resnet50-anjuu-512.csv.gz\"\n",
    "print(valid_filename)\n",
    "sub_filename = \"../submissions/Bac/avg_resnet50-anjuu-512.csv.gz\" # LB 0.067\n",
    "print(sub_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn META_NUMBER_IMAGES_IN_USE ON/OFF to create 2 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:25.898868Z",
     "start_time": "2019-10-28T16:33:25.874459Z"
    }
   },
   "outputs": [],
   "source": [
    "META_NUMBER_IMAGES_IN_USE = True\n",
    "NUMBER_FOLDS = 6 # 6 for if we include the stage 1 set\n",
    "print(NUMBER_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:25.927385Z",
     "start_time": "2019-10-28T16:33:25.902644Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_post_filename = valid_filename.replace(\".csv\", \n",
    "                                \"_post_{}.csv\".format(\"with_meta\" if META_NUMBER_IMAGES_IN_USE else \"wo_meta\"))\n",
    "print(valid_post_filename)\n",
    "sub_post_filename = sub_filename.replace(\".csv\", \n",
    "                                \"_post_{}.csv\".format(\"with_meta\" if META_NUMBER_IMAGES_IN_USE else \"wo_meta\"))\n",
    "print(sub_post_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:25.955446Z",
     "start_time": "2019-10-28T16:33:25.930517Z"
    }
   },
   "outputs": [],
   "source": [
    "predefined_features = ['p', 'rank', 'rank_inv', 'rank_max', 'p1', 'p1_std', 'p1_skew', 'p1_list_std', \n",
    "                        'p1_inv', 'p1_inv_std', 'p1_inv_skew', 'p1_inv_list_std', 'rank_perc', 'p_next', 'p_prev']\n",
    "\n",
    "excluded_meta_features = [] if META_NUMBER_IMAGES_IN_USE else [c for c in predefined_features if \"rank\" in c]\n",
    "print(excluded_meta_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:25.984866Z",
     "start_time": "2019-10-28T16:33:25.959734Z"
    }
   },
   "outputs": [],
   "source": [
    "models_h2o_dir = \"../models_h2o/{}_{}/\".format(dp_model, \"with_meta\" if META_NUMBER_IMAGES_IN_USE else \"wo_meta\")\n",
    "print(models_h2o_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:26.168385Z",
     "start_time": "2019-10-28T16:33:25.988761Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p $models_h2o_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:26.177274Z",
     "start_time": "2019-10-28T16:33:26.172748Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dir = \"../input/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:35.655418Z",
     "start_time": "2019-10-28T16:33:26.179784Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import h2o\n",
    "# from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "# from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "np.random.seed(2019)\n",
    "\n",
    "import datetime as dt\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:35.668849Z",
     "start_time": "2019-10-28T16:33:35.658783Z"
    }
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 5, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:35.691181Z",
     "start_time": "2019-10-28T16:33:35.670347Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "pd.options.display.float_format = '{:,.5f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:37.204704Z",
     "start_time": "2019-10-28T16:33:35.694602Z"
    }
   },
   "outputs": [],
   "source": [
    "# usecols = ['ID', 'Label', 'Sub_type', 'PatientID']\n",
    "\n",
    "usecols = [\"sop_instance_uid\", \"patient_id\", \"study_instance_uid\"] + \\\n",
    "    [\"image_position_patient_2\"]\n",
    "\n",
    "train_metadata = pd.read_csv(input_dir + \"df_dicom_metadata_train.csv\", usecols = usecols)\n",
    "print(train_metadata.shape)\n",
    "train_metadata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:39.397690Z",
     "start_time": "2019-10-28T16:33:37.207853Z"
    }
   },
   "outputs": [],
   "source": [
    "train_label = pd.read_csv(input_dir + \"stage_1_train.csv\", usecols = None)\n",
    "print(train_label.shape)\n",
    "train_label.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:47.974846Z",
     "start_time": "2019-10-28T16:33:39.399070Z"
    }
   },
   "outputs": [],
   "source": [
    "train_label[\"sop_instance_uid\"] = train_label[\"ID\"].apply(lambda x: \"ID_\" + x.split(\"_\")[1])\n",
    "train_label[\"SubType\"] = train_label[\"ID\"].apply(lambda x: x.split(\"_\")[2])\n",
    "train_label.head(3)\n",
    "\n",
    "train_label = pd.pivot_table(train_label, values='Label', index=['sop_instance_uid'], columns=['SubType'], aggfunc=np.max).reset_index()\n",
    "train_label.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:33:48.533347Z",
     "start_time": "2019-10-28T16:33:47.978143Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_metadata.shape)\n",
    "train_metadata = pd.merge(train_metadata, train_label, on=\"sop_instance_uid\")\n",
    "print(train_metadata.shape)\n",
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.930Z"
    }
   },
   "outputs": [],
   "source": [
    "train_patients = train_metadata.groupby([\"patient_id\"])[\"any\"].max().to_frame(\"any\").reset_index()\n",
    "train_patients.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.932Z"
    }
   },
   "outputs": [],
   "source": [
    "train_patients[\"any\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.936Z"
    }
   },
   "outputs": [],
   "source": [
    "test_metadata = pd.read_csv(input_dir + \"df_dicom_metadata_test.csv\", usecols = usecols)\n",
    "test_metadata[\"set\"] = 0\n",
    "print(test_metadata.shape)\n",
    "test_metadata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.939Z"
    }
   },
   "outputs": [],
   "source": [
    "test_metadata[\"patient_id\"].unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.942Z"
    }
   },
   "outputs": [],
   "source": [
    "valid = pd.read_csv(valid_filename, compression=\"gzip\" if \".gz\" in valid_filename else None)\n",
    "print(valid.shape)\n",
    "valid.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.945Z"
    }
   },
   "outputs": [],
   "source": [
    "if (len(valid.columns) == 2) and (\"Label\" in valid.columns):\n",
    "    print(\"Pivoting the validation set ...\")\n",
    "    valid[\"sop_instance_uid\"] = valid[\"ID\"].apply(lambda x: \"ID_\" + x.split(\"_\")[1])\n",
    "    valid[\"SubType\"] = valid[\"ID\"].apply(lambda x: x.split(\"_\")[2])\n",
    "    valid.head(3)\n",
    "\n",
    "    valid = pd.pivot_table(valid, values='Label', index=['sop_instance_uid'], columns=['SubType'], aggfunc=np.max).reset_index()\n",
    "valid.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.948Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.951Z"
    }
   },
   "outputs": [],
   "source": [
    "p_cols_map = { c : \"p_{}\".format(c) for c in cols}\n",
    "valid.rename(columns=p_cols_map, inplace=True)\n",
    "valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.954Z"
    }
   },
   "outputs": [],
   "source": [
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.957Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_cols = [\"sop_instance_uid\", \"patient_id\", \"study_instance_uid\",\"image_position_patient_2\"]\n",
    "\n",
    "df = pd.merge(valid[[\"sop_instance_uid\"] + [\"p_{}\".format(c) for c in cols]], \n",
    "              train_metadata[meta_cols + cols], on=[\"sop_instance_uid\"])\n",
    "print(valid.shape, df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.960Z"
    }
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators import H2OGeneralizedLinearEstimator\n",
    "from h2o.estimators import H2ONaiveBayesEstimator\n",
    "from h2o.estimators import H2OStackedEnsembleEstimator\n",
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "from h2o.grid import H2OGridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.962Z"
    }
   },
   "outputs": [],
   "source": [
    "port = 54321\n",
    "h2o.init(port=port,max_mem_size=\"100G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.966Z"
    }
   },
   "outputs": [],
   "source": [
    "id_col = \"sop_instance_uid\"\n",
    "key_cols = [\"patient_id\", \"study_instance_uid\"]\n",
    "pos_col = \"image_position_patient_2\"\n",
    "ordered_cols = key_cols + [pos_col]\n",
    "\n",
    "df.sort_values(ordered_cols, ascending=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.970Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_features(df, label_col=\"any\", WIN3 = True, is_train=True):\n",
    "    selected_cols = [id_col] + ordered_cols + [\"p_{}\".format(label_col)]\n",
    "    if is_train:\n",
    "        selected_cols.append(label_col)\n",
    "    \n",
    "    df_sub = df[selected_cols].copy()\n",
    "    df_sub.rename(columns={\"p_{}\".format(label_col): \"p\"}, inplace=True)\n",
    "\n",
    "    df_sub.sort_values(ordered_cols, ascending=True, inplace=True)\n",
    "    df_sub.reset_index(drop=True, inplace=True)\n",
    "    df_sub.head(10)\n",
    "\n",
    "    # Rank\n",
    "    df_sub[\"rank\"] = df_sub.groupby(key_cols)[pos_col].rank(ascending=True).astype(int)\n",
    "    df_sub[\"rank_inv\"] = df_sub.groupby(key_cols)[pos_col].rank(ascending=False).astype(int)\n",
    "    df_sub_grouped = df_sub.groupby(key_cols)[\"rank\"].count() #.agg({'Label':'max'})\n",
    "    df_sub_grouped = df_sub_grouped.to_frame(\"rank_max\").reset_index()\n",
    "    df_sub = pd.merge(df_sub, df_sub_grouped, how='left', on=key_cols)\n",
    "    df_sub.head()\n",
    "\n",
    "    def list_std(x):\n",
    "        return np.std(x[x>0])\n",
    "\n",
    "    # Features\n",
    "    df_sub['p1'] = df_sub.groupby(key_cols)['p'].apply(lambda x: x.shift().expanding().mean())\n",
    "    df_sub['p1_std'] = df_sub.groupby(key_cols)['p'].apply(lambda x: x.shift().expanding().std())\n",
    "    df_sub['p1_skew'] = df_sub.groupby(key_cols)['p'].apply(lambda x: x.shift().expanding().skew())\n",
    "    df_sub['p1_list_std'] = df_sub.groupby(key_cols)['p'].apply(lambda x: \\\n",
    "                                                        x.shift().expanding().apply(lambda x: list_std(x), 'raw=False'))\n",
    "\n",
    "    df_sub['p1_inv'] = df_sub.sort_values(key_cols + [\"rank_inv\"]).groupby(key_cols)['p'].apply(lambda x: x.shift().expanding().mean())\n",
    "    df_sub['p1_inv_std'] = df_sub.sort_values(key_cols + [\"rank_inv\"]).groupby(key_cols)['p'].apply(lambda x: x.shift().expanding().std())\n",
    "    df_sub['p1_inv_skew'] = df_sub.sort_values(key_cols + [\"rank_inv\"]).groupby(key_cols)['p'].apply(lambda x: x.shift().expanding().skew())\n",
    "    df_sub['p1_inv_list_std'] = df_sub.sort_values(key_cols + [\"rank_inv\"]).groupby(key_cols)['p'].apply(lambda x: \\\n",
    "                                                        x.shift().expanding().apply(lambda x: list_std(x), 'raw=False'))\n",
    "\n",
    "    df_sub[\"rank_perc\"] = df_sub[\"rank\"] / df_sub[\"rank_max\"] \n",
    "    df_sub.head()\n",
    "\n",
    "    if WIN3:\n",
    "        df_sub[\"p_next\"] = df_sub[\"p\"].shift(-1)\n",
    "        df_sub[\"p_prev\"] = df_sub[\"p\"].shift(+1)\n",
    "    df_sub.head()\n",
    "    return df_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling for each label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.974Z"
    }
   },
   "outputs": [],
   "source": [
    "df_stacking_patients = pd.read_csv(patients_stacking_splits_filename)\n",
    "print(df_stacking_patients.shape[0])\n",
    "print(df_stacking_patients[\"stacking_fold\"].value_counts())\n",
    "df_stacking_patients.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.976Z"
    }
   },
   "outputs": [],
   "source": [
    "all_valid_patients = set(df_stacking_patients[df_stacking_patients[\"stacking_fold\"] < NUMBER_FOLDS][\"patient_id\"])\n",
    "print(len(all_valid_patients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modeling\n",
    "\n",
    "all_kpi_valid = {}\n",
    "valid_h2o_parts = []\n",
    "for label_col in cols[:]: # \"any\"\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Processing label {} ...\".format(label_col))\n",
    "\n",
    "    # Feature creation\n",
    "    print(\"Feature creating ...\")\n",
    "    df_sub = create_features(df, label_col)\n",
    "\n",
    "    excluded_cols = [id_col] + ordered_cols + cols\n",
    "    #print(excluded_cols)\n",
    "    feature_cols = [c for c in df_sub.columns if c not in excluded_cols]\n",
    "    # Number of images per study\n",
    "    feature_cols = [c for c in feature_cols if c not in excluded_meta_features]\n",
    "    print(\"Features:\", feature_cols)\n",
    "\n",
    "    # Drop NA\n",
    "    df_ok = df_sub.dropna()\n",
    "    print(df_ok.shape, df_sub.shape)\n",
    "\n",
    "    fold_kpi_valid = {}\n",
    "    for fold in range(NUMBER_FOLDS):\n",
    "        # Fold\n",
    "        patients_valid = set(df_stacking_patients[df_stacking_patients[\"stacking_fold\"] == fold][\"patient_id\"])\n",
    "        patients_train = all_valid_patients - patients_valid\n",
    "        print(fold, len(patients_train), len(patients_valid))\n",
    "    \n",
    "        # Split\n",
    "        data_train = h2o.H2OFrame(df_ok[df_ok[\"patient_id\"].isin(patients_train)]) \n",
    "        data_valid = h2o.H2OFrame(df_ok[df_ok[\"patient_id\"].isin(patients_valid)]) \n",
    "\n",
    "        data_train[\"TARGET\"] = data_train[label_col].asfactor()\n",
    "        data_valid[\"TARGET\"] = data_valid[label_col].asfactor()\n",
    "\n",
    "        # Modeling\n",
    "        model_id = 'model_{}_{}'.format(label_col, fold)\n",
    "        print(\"Modeling {} ...\".format(model_id))\n",
    "\n",
    "        model = H2OGradientBoostingEstimator(model_id = model_id, ntrees=100, nfolds=5,\n",
    "                                                stopping_rounds=5, stopping_metric=\"logloss\", stopping_tolerance=0.02,\n",
    "                                                seed=2019)\n",
    "\n",
    "        model.train(x=feature_cols, y=\"TARGET\", training_frame=data_train, validation_frame=data_valid)\n",
    "\n",
    "        # Save\n",
    "        model_path = h2o.save_model(model=model, path=models_h2o_dir + 'model_{}'.format(label_col), force=True)\n",
    "        print(model_path)\n",
    "\n",
    "        # Feature importance\n",
    "        df_fi = model._model_json['output']['variable_importances'].as_data_frame()\n",
    "        print(df_fi.head(5))\n",
    "\n",
    "        # model.varimp_plot();\n",
    "\n",
    "        # Validation\n",
    "        hf_y_pred = model.predict(data_valid)\n",
    "        df_y_pred = (data_valid[[\"sop_instance_uid\"] + key_cols + [\"p\", label_col]]).cbind(hf_y_pred).as_data_frame()\n",
    "        print(df_y_pred.shape)\n",
    "        df_y_pred.head(3)\n",
    "\n",
    "        loss0 = log_loss(y_true=df_y_pred[label_col], y_pred=df_y_pred[\"p\"])\n",
    "        loss1 = log_loss(y_true=df_y_pred[label_col], y_pred=df_y_pred[\"p1\"])\n",
    "        print(\"Fold {}: origin loss = {}, new loss = {}\".format(fold, loss0, loss1)) # 0.11695034962163679, 0.11695034962163679\n",
    "\n",
    "        fold_kpi_valid[fold] = (loss0, loss1, model_path)\n",
    "        # Append\n",
    "        df_y_pred[\"ID\"] = df_y_pred[\"sop_instance_uid\"] + \"_{}\".format(label_col)\n",
    "        valid_h2o_parts.append(df_y_pred[[\"ID\", \"p1\"]])\n",
    "        #\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "    all_kpi_valid[label_col] = fold_kpi_valid\n",
    "len(valid_h2o_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.983Z"
    }
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.986Z"
    }
   },
   "outputs": [],
   "source": [
    "all_kpi_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.989Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_h2o = pd.concat(valid_h2o_parts, axis=0)\n",
    "print(valid_h2o.shape)\n",
    "print(valid_h2o.head(3))\n",
    "valid_h2o = valid_h2o.groupby(\"ID\")[\"p1\"].mean().to_frame(\"p1\").reset_index()\n",
    "valid_h2o.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.992Z"
    }
   },
   "outputs": [],
   "source": [
    "weighted_mean = lambda x: (x[0] + 3*x[1] + x[2]) / 5\n",
    "\n",
    "def smooth_interpolate_sub(sub_filename, metadata_filename = \"df_dicom_metadata_train.csv\"):\n",
    "    sub = pd.read_csv(sub_filename, usecols = None, compression=\"gzip\" if \".gz\" in sub_filename else None)\n",
    "    \n",
    "    sub[\"sop_instance_uid\"] = sub[\"ID\"].apply(lambda x: \"ID_\" + x.split(\"_\")[1])\n",
    "    sub[\"cls\"] = sub[\"ID\"].apply(lambda x: x.split(\"_\")[2])\n",
    "\n",
    "    key_cols = ['patient_id', 'study_instance_uid', 'image_position_patient_2']\n",
    "    usecols = ['sop_instance_uid'] + key_cols\n",
    "    meta = pd.read_csv(metadata_filename, usecols = usecols)\n",
    "    \n",
    "    sub2 = sub.copy()\n",
    "    for c in cols:\n",
    "        print(c)\n",
    "        sub_any = sub[sub[\"cls\"] == c].copy()\n",
    "        sub_any = pd.merge(sub_any, meta, on=\"sop_instance_uid\")\n",
    "        sub_any.sort_values(key_cols, inplace=True)\n",
    "        sub_any.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        sub_any[\"f2_{}\".format(c)] = sub_any.groupby('study_instance_uid')['Label'].rolling(3).apply(weighted_mean).reset_index(0,drop=True)\n",
    "        sub_any[\"f3_{}\".format(c)] = sub_any[\"f2_{}\".format(c)].shift(-1) # sub_any[\"f2_{}\".format(c)].shift(-1)\n",
    "\n",
    "        sub2 = pd.merge(sub2, sub_any[[\"ID\", \"f3_{}\".format(c)]], on=\"ID\", how=\"left\") \n",
    "    \n",
    "    sub2[\"f3_Label\"] = sub2[\"Label\"]\n",
    "    for c in cols:\n",
    "        print(c)\n",
    "        sub2.loc[~sub2[\"f3_{}\".format(c)].isna(), \"f3_Label\"] = \\\n",
    "            sub2.loc[~sub2[\"f3_{}\".format(c)].isna(), \"f3_{}\".format(c)]\n",
    "    \n",
    "    return sub2, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.995Z"
    }
   },
   "outputs": [],
   "source": [
    "print(valid_filename)\n",
    "\n",
    "valid_smooth, meta = smooth_interpolate_sub(sub_filename = valid_filename,\n",
    "       metadata_filename = input_dir + \"df_dicom_metadata_train.csv\")\n",
    "valid_smooth.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination (supported labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:25.998Z"
    }
   },
   "outputs": [],
   "source": [
    "#supported_labels = [\"any\"]\n",
    "all_supported_labels = cols\n",
    "print(all_supported_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.001Z"
    }
   },
   "outputs": [],
   "source": [
    "supported_labels = []\n",
    "for label_col in all_supported_labels: #label_col = \"subdural\"\n",
    "    print(\"----------------------------------------\")\n",
    "    \n",
    "    kpi = all_kpi_valid[label_col]\n",
    "    #print(kpi)\n",
    "    kpi_0 = 0\n",
    "    kpi_1 = 0\n",
    "    for fold in range(NUMBER_FOLDS):\n",
    "        kpi_0 = kpi_0 + kpi[fold][0]\n",
    "        kpi_1 = kpi_1 + kpi[fold][1]\n",
    "    if(kpi_0 <= kpi_1):\n",
    "        print(\"NOT OK => ignore {}\".format(label_col))\n",
    "        continue\n",
    "    else:\n",
    "        print(label_col, kpi_0/5, kpi_1/5)\n",
    "        supported_labels.append(label_col)\n",
    "print(supported_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.003Z"
    }
   },
   "outputs": [],
   "source": [
    "print(valid_h2o.shape)\n",
    "pd.merge(valid_smooth[[\"ID\", \"Label\", \"cls\", \"f3_Label\"]], valid_h2o, on=\"ID\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.008Z"
    }
   },
   "outputs": [],
   "source": [
    "final_valid = pd.merge(valid_smooth[[\"ID\", \"Label\", \"cls\", \"f3_Label\"]], valid_h2o, on=\"ID\", how=\"left\")\n",
    "print(valid_smooth.shape, final_valid.shape)\n",
    "print(final_valid.isnull().sum())\n",
    "final_valid.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.012Z"
    }
   },
   "outputs": [],
   "source": [
    "final_valid[\"Label\"] = final_valid[\"f3_Label\"] \n",
    "supported_index = (~final_valid[\"p1\"].isna()) & final_valid[\"cls\"].isin(supported_labels)\n",
    "\n",
    "final_valid.loc[supported_index, \"Label\"] = final_valid.loc[supported_index, \"p1\"]\n",
    "final_valid.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.014Z"
    }
   },
   "outputs": [],
   "source": [
    "print(valid_post_filename, final_valid.shape)\n",
    "final_valid[[\"ID\", \"Label\"]].to_csv(valid_post_filename, compression=\"gzip\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.017Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sub_filename)\n",
    "sub = pd.read_csv(sub_filename, compression=\"gzip\" if \".gz\" in sub_filename else None)\n",
    "print(sub.shape)\n",
    "sub.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.020Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sub[sub[\"ID\"].isin([\"ID_28fbab7eb_epidural\"])])\n",
    "sub.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.023Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv(input_dir + \"stage_1_sample_submission.csv\")\n",
    "print(sample.shape)\n",
    "sample.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.026Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.merge(sample[[\"ID\"]], sub, on=\"ID\") # Ordered\n",
    "print(sub.shape)\n",
    "sub.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.029Z"
    }
   },
   "outputs": [],
   "source": [
    "sub[\"sop_instance_uid\"] = sub[\"ID\"].apply(lambda x: \"ID_\" + x.split(\"_\")[1])\n",
    "sub[\"SubType\"] = sub[\"ID\"].apply(lambda x: x.split(\"_\")[2])\n",
    "sub.head(3)\n",
    "\n",
    "sub = pd.pivot_table(sub, values='Label', index=['sop_instance_uid'], columns=['SubType'], aggfunc=np.max).reset_index()\n",
    "sub.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.031Z"
    }
   },
   "outputs": [],
   "source": [
    "print(meta_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.035Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_h2o_parts = []\n",
    "for label_col in supported_labels: #label_col = \"subdural\"\n",
    "    print(\"----------------------------------------\")    \n",
    "    kpi = all_kpi_valid[label_col]\n",
    "        \n",
    "    # Feature creation\n",
    "    print(\"Feature creating ...\")\n",
    "    df_sub_temp = pd.merge(sub[[\"sop_instance_uid\", label_col]].rename(columns={label_col: \"p_{}\".format(label_col)}), \\\n",
    "                 test_metadata[meta_cols], on=\"sop_instance_uid\")\n",
    "    df_sub = create_features(df_sub_temp, label_col, is_train=False)\n",
    "    df_sub.head(3)\n",
    "\n",
    "    df_sub_ok = df_sub.dropna().copy()\n",
    "    print(df_sub_ok.shape, df_sub.shape)\n",
    "    hf_sub = h2o.H2OFrame(df_sub_ok) \n",
    "        \n",
    "    for fold in range(NUMBER_FOLDS):\n",
    "        model_path = kpi[fold][2]\n",
    "        print(\"Processing label {} ...\".format(label_col))\n",
    "        print(model_path)\n",
    "\n",
    "        model_sub = h2o.load_model(path=model_path)\n",
    "\n",
    "        hf_y_sub_pred = model_sub.predict(hf_sub)\n",
    "        #df_y_sub_pred = (hf_sub[[\"sop_instance_uid\"]+key_cols].cbind(hf_y_sub_pred)).as_data_frame()\n",
    "        df_y_sub_pred = (hf_sub[[\"sop_instance_uid\"]].cbind(hf_y_sub_pred)).as_data_frame()\n",
    "        print(\"Output:\", df_y_sub_pred.shape, df_y_sub_pred.columns)\n",
    "\n",
    "        df_y_sub_pred[\"ID\"] = df_y_sub_pred[\"sop_instance_uid\"] + \"_{}\".format(label_col)\n",
    "\n",
    "        # Append\n",
    "        sub_h2o_parts.append(df_y_sub_pred[[\"ID\", \"p1\"]])\n",
    "    \n",
    "print(len(sub_h2o_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.037Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_h2o = pd.concat(sub_h2o_parts, axis=0)\n",
    "print(sub_h2o.shape)\n",
    "print(sub_h2o.head(3))\n",
    "sub_h2o = sub_h2o.groupby(\"ID\")[\"p1\"].mean().to_frame(\"p1\").reset_index()\n",
    "sub_h2o.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.039Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_h2o[sub_h2o[\"ID\"].isin([\"ID_584e7fced_any\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth and Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.041Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sub_filename)\n",
    "sub_smooth, meta = smooth_interpolate_sub(sub_filename = sub_filename,\n",
    "       metadata_filename = input_dir + \"df_dicom_metadata_test.csv\")\n",
    "sub_smooth.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.043Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sub_h2o.shape)\n",
    "pd.merge(sub_smooth[[\"ID\", \"Label\", \"cls\", \"f3_Label\"]], sub_h2o, on=\"ID\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.046Z"
    }
   },
   "outputs": [],
   "source": [
    "final_sub = pd.merge(sub_smooth[[\"ID\", \"Label\", \"cls\", \"f3_Label\"]], sub_h2o, on=\"ID\", how=\"left\")\n",
    "print(sub_smooth.shape, final_sub.shape)\n",
    "print(final_sub.isnull().sum())\n",
    "final_sub.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.048Z"
    }
   },
   "outputs": [],
   "source": [
    "final_sub[\"Label\"] = final_sub[\"f3_Label\"] \n",
    "supported_index = (~final_sub[\"p1\"].isna()) & final_sub[\"cls\"].isin(supported_labels)\n",
    "\n",
    "final_sub.loc[supported_index, \"Label\"] = final_sub.loc[supported_index, \"p1\"]\n",
    "final_sub.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.051Z"
    }
   },
   "outputs": [],
   "source": [
    "sample.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.053Z"
    }
   },
   "outputs": [],
   "source": [
    "final_sub = pd.merge(sample[[\"ID\"]], final_sub, on=\"ID\")\n",
    "print(final_sub.shape)\n",
    "final_sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.055Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sub_post_filename)\n",
    "final_sub[[\"ID\", \"Label\"]].to_csv(sub_post_filename, compression=\"gzip\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-28T16:33:26.057Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sub_post_filename)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpu36]",
   "language": "python",
   "name": "conda-env-gpu36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
